{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "import random\n",
    "from decimal import Decimal\n",
    "sc = pyspark.SparkContext()\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unmarshal_dynamodb_json(node):\n",
    "    data = dict({})\n",
    "    data['M'] = node\n",
    "    return _unmarshal_value(data)\n",
    "\n",
    "\n",
    "def _unmarshal_value(node):\n",
    "    if type(node) is not dict:\n",
    "        return node\n",
    "\n",
    "    for key, value in node.items():\n",
    "        # S – String - return string\n",
    "        # N – Number - return int or float (if includes '.')\n",
    "        # B – Binary - not handled\n",
    "        # BOOL – Boolean - return Bool\n",
    "        # NULL – Null - return None\n",
    "        # M – Map - return a dict\n",
    "        # L – List - return a list\n",
    "        # SS – String Set - not handled\n",
    "        # NN – Number Set - not handled\n",
    "        # BB – Binary Set - not handled\n",
    "        key = key.lower()\n",
    "        if key == 'bool':\n",
    "            return value\n",
    "        if key == 'null':\n",
    "            return None\n",
    "        if key == 's':\n",
    "            return value\n",
    "        if key == 'n':\n",
    "            if '.' in str(value):\n",
    "                return Decimal(value)\n",
    "            return int(value)\n",
    "        if key in ['m', 'l']:\n",
    "            if key == 'm':\n",
    "                data = {}\n",
    "                for key1, value1 in value.items():\n",
    "                    if key1.lower() == 'l':\n",
    "                        data = [_unmarshal_value(n) for n in value1]\n",
    "                    else:\n",
    "                        if type(value1) is not dict:\n",
    "                            return _unmarshal_value(value)\n",
    "                        data[key1] = _unmarshal_value(value1)\n",
    "                return data\n",
    "            data = []\n",
    "            for item in value:\n",
    "                data.append(_unmarshal_value(item))\n",
    "            return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "log_txt=sc.textFile(\"parsing.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfout = log_txt.map(lambda j: unmarshal_dynamodb_json(json.loads(j)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'updated': u'2017-09-21T20:00:53.065618', u'is_fake_company': False, u'tenant_id': 1, u'is_shiftgig_company': False, u'is_status_company': True, u'company_id': 251, u'company_name': u'Sodexo | Northwestern University:McCormick School of Engineering', u'is_geo_enabled_company': False, u'company_market_id': 1, u'is_dead_company': False}\n"
     ]
    }
   ],
   "source": [
    "print(dfout.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dfout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{u'company_id': 251,\n",
       "  u'company_market_id': 1,\n",
       "  u'company_name': u'Sodexo | Northwestern University:McCormick School of Engineering',\n",
       "  u'is_dead_company': False,\n",
       "  u'is_fake_company': False,\n",
       "  u'is_geo_enabled_company': False,\n",
       "  u'is_shiftgig_company': False,\n",
       "  u'is_status_company': True,\n",
       "  u'tenant_id': 1,\n",
       "  u'updated': u'2017-09-21T20:00:53.065618'},\n",
       " {u'company_id': 3757,\n",
       "  u'company_market_id': 10,\n",
       "  u'company_name': u'The Victory Ship Inc.',\n",
       "  u'is_dead_company': False,\n",
       "  u'is_fake_company': False,\n",
       "  u'is_geo_enabled_company': False,\n",
       "  u'is_shiftgig_company': False,\n",
       "  u'is_status_company': True,\n",
       "  u'tenant_id': 1,\n",
       "  u'updated': u'2018-04-13T09:38:34.977732'},\n",
       " {u'company_id': 32,\n",
       "  u'company_market_id': 1,\n",
       "  u'company_name': u'Do Not Use- Levy Restaurants',\n",
       "  u'is_dead_company': True,\n",
       "  u'is_fake_company': False,\n",
       "  u'is_geo_enabled_company': False,\n",
       "  u'is_shiftgig_company': False,\n",
       "  u'is_status_company': True,\n",
       "  u'tenant_id': 1,\n",
       "  u'updated': u'2017-09-21T20:00:53.065618'},\n",
       " {u'company_id': 3517,\n",
       "  u'company_market_id': 28,\n",
       "  u'company_name': u'Initech NY',\n",
       "  u'is_dead_company': False,\n",
       "  u'is_fake_company': True,\n",
       "  u'is_geo_enabled_company': False,\n",
       "  u'is_shiftgig_company': False,\n",
       "  u'is_status_company': True,\n",
       "  u'tenant_id': 3,\n",
       "  u'updated': u'2017-10-29T12:43:41.415538'},\n",
       " {u'company_id': 322,\n",
       "  u'company_market_id': 3,\n",
       "  u'company_name': u'Compass | Wolfgang | Nasher Sculpture Museum',\n",
       "  u'is_dead_company': False,\n",
       "  u'is_fake_company': False,\n",
       "  u'is_geo_enabled_company': False,\n",
       "  u'is_shiftgig_company': False,\n",
       "  u'is_status_company': True,\n",
       "  u'tenant_id': 1,\n",
       "  u'updated': u'2017-09-21T20:00:53.065618'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfout.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/opt/apache-spark/libexec/python/pyspark/sql/session.py:360: UserWarning: Using RDD of dict to inferSchema is deprecated. Use pyspark.sql.Row instead\n",
      "  warnings.warn(\"Using RDD of dict to inferSchema is deprecated. \"\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "df_csv = sqlContext.createDataFrame(dfout, ['company_id', 'company_market_id','company_name','is_dead_company','is_fake_company','is_geo_enabled_company','is_shiftgig_company','is_status_company','tenant_id','updated'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv.coalesce(1).write.format('com.databricks.spark.csv').options(header='true').save('csv_out.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
