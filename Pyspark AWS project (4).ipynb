{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "import random\n",
    "from decimal import Decimal\n",
    "sc = pyspark.SparkContext()\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unmarshal_dynamodb_json(node):\n",
    "    data = dict({})\n",
    "    data['M'] = node\n",
    "    return _unmarshal_value(data)\n",
    "\n",
    "\n",
    "def _unmarshal_value(node):\n",
    "    if type(node) is not dict:\n",
    "        return node\n",
    "\n",
    "    for key, value in node.items():\n",
    "        # S – String - return string\n",
    "        # N – Number - return int or float (if includes '.')\n",
    "        # B – Binary - not handled\n",
    "        # BOOL – Boolean - return Bool\n",
    "        # NULL – Null - return None\n",
    "        # M – Map - return a dict\n",
    "        # L – List - return a list\n",
    "        # SS – String Set - not handled\n",
    "        # NN – Number Set - not handled\n",
    "        # BB – Binary Set - not handled\n",
    "        key = key.lower()\n",
    "        if key == 'bool':\n",
    "            return value\n",
    "        if key == 'null':\n",
    "            return None\n",
    "        if key == 's':\n",
    "            return value\n",
    "        if key == 'n':\n",
    "            if '.' in str(value):\n",
    "                return Decimal(value)\n",
    "            return int(value)\n",
    "        if key in ['m', 'l']:\n",
    "            if key == 'm':\n",
    "                data = {}\n",
    "                for key1, value1 in value.items():\n",
    "                    if key1.lower() == 'l':\n",
    "                        data = [_unmarshal_value(n) for n in value1]\n",
    "                    else:\n",
    "                        if type(value1) is not dict:\n",
    "                            return _unmarshal_value(value)\n",
    "                        data[key1] = _unmarshal_value(value1)\n",
    "                return data\n",
    "            data = []\n",
    "            for item in value:\n",
    "                data.append(_unmarshal_value(item))\n",
    "            return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "log_txt=sc.textFile(\"parsing.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "dfout = log_txt.map(lambda j: unmarshal_dynamodb_json(json.loads(j)))\n",
    "# sqlContext.udf.register('unmarshal_dynamodb_json', )\n",
    "# dfout = udf(unmarshal_dynamodb_json, StructType())\n",
    "\n",
    "log_txt1=sqlContext.read.text(\"parsing.txt\")\n",
    "df = sqlContext.read.json(log_txt1.rdd.map(lambda j: json.dumps(unmarshal_dynamodb_json(json.loads(j['value'])))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'updated': u'2017-09-21T20:00:53.065618', u'is_fake_company': False, u'tenant_id': 1, u'is_shiftgig_company': False, u'is_status_company': True, u'company_id': 251, u'company_name': u'Sodexo | Northwestern University:McCormick School of Engineering', u'is_geo_enabled_company': False, u'company_market_id': 1, u'is_dead_company': False}\n"
     ]
    }
   ],
   "source": [
    "print(dfout.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(company_id=251, company_market_id=1, company_name=u'Sodexo | Northwestern University:McCormick School of Engineering', is_dead_company=False, is_fake_company=False, is_geo_enabled_company=False, is_shiftgig_company=False, is_status_company=True, tenant_id=1, updated=u'2017-09-21T20:00:53.065618')\n"
     ]
    }
   ],
   "source": [
    "print(df.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dfout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{u'company_id': 251,\n",
       "  u'company_market_id': 1,\n",
       "  u'company_name': u'Sodexo | Northwestern University:McCormick School of Engineering',\n",
       "  u'is_dead_company': False,\n",
       "  u'is_fake_company': False,\n",
       "  u'is_geo_enabled_company': False,\n",
       "  u'is_shiftgig_company': False,\n",
       "  u'is_status_company': True,\n",
       "  u'tenant_id': 1,\n",
       "  u'updated': u'2017-09-21T20:00:53.065618'},\n",
       " {u'company_id': 3757,\n",
       "  u'company_market_id': 10,\n",
       "  u'company_name': u'The Victory Ship Inc.',\n",
       "  u'is_dead_company': False,\n",
       "  u'is_fake_company': False,\n",
       "  u'is_geo_enabled_company': False,\n",
       "  u'is_shiftgig_company': False,\n",
       "  u'is_status_company': True,\n",
       "  u'tenant_id': 1,\n",
       "  u'updated': u'2018-04-13T09:38:34.977732'},\n",
       " {u'company_id': 32,\n",
       "  u'company_market_id': 1,\n",
       "  u'company_name': u'Do Not Use- Levy Restaurants',\n",
       "  u'is_dead_company': True,\n",
       "  u'is_fake_company': False,\n",
       "  u'is_geo_enabled_company': False,\n",
       "  u'is_shiftgig_company': False,\n",
       "  u'is_status_company': True,\n",
       "  u'tenant_id': 1,\n",
       "  u'updated': u'2017-09-21T20:00:53.065618'},\n",
       " {u'company_id': 3517,\n",
       "  u'company_market_id': 28,\n",
       "  u'company_name': u'Initech NY',\n",
       "  u'is_dead_company': False,\n",
       "  u'is_fake_company': True,\n",
       "  u'is_geo_enabled_company': False,\n",
       "  u'is_shiftgig_company': False,\n",
       "  u'is_status_company': True,\n",
       "  u'tenant_id': 3,\n",
       "  u'updated': u'2017-10-29T12:43:41.415538'},\n",
       " {u'company_id': 322,\n",
       "  u'company_market_id': 3,\n",
       "  u'company_name': u'Compass | Wolfgang | Nasher Sculpture Museum',\n",
       "  u'is_dead_company': False,\n",
       "  u'is_fake_company': False,\n",
       "  u'is_geo_enabled_company': False,\n",
       "  u'is_shiftgig_company': False,\n",
       "  u'is_status_company': True,\n",
       "  u'tenant_id': 1,\n",
       "  u'updated': u'2017-09-21T20:00:53.065618'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfout.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(company_id=251, company_market_id=1, company_name=u'Sodexo | Northwestern University:McCormick School of Engineering', is_dead_company=False, is_fake_company=False, is_geo_enabled_company=False, is_shiftgig_company=False, is_status_company=True, tenant_id=1, updated=u'2017-09-21T20:00:53.065618'),\n",
       " Row(company_id=3757, company_market_id=10, company_name=u'The Victory Ship Inc.', is_dead_company=False, is_fake_company=False, is_geo_enabled_company=False, is_shiftgig_company=False, is_status_company=True, tenant_id=1, updated=u'2018-04-13T09:38:34.977732'),\n",
       " Row(company_id=32, company_market_id=1, company_name=u'Do Not Use- Levy Restaurants', is_dead_company=True, is_fake_company=False, is_geo_enabled_company=False, is_shiftgig_company=False, is_status_company=True, tenant_id=1, updated=u'2017-09-21T20:00:53.065618'),\n",
       " Row(company_id=3517, company_market_id=28, company_name=u'Initech NY', is_dead_company=False, is_fake_company=True, is_geo_enabled_company=False, is_shiftgig_company=False, is_status_company=True, tenant_id=3, updated=u'2017-10-29T12:43:41.415538'),\n",
       " Row(company_id=322, company_market_id=3, company_name=u'Compass | Wolfgang | Nasher Sculpture Museum', is_dead_company=False, is_fake_company=False, is_geo_enabled_company=False, is_shiftgig_company=False, is_status_company=True, tenant_id=1, updated=u'2017-09-21T20:00:53.065618')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/opt/apache-spark/libexec/python/pyspark/sql/session.py:360: UserWarning: Using RDD of dict to inferSchema is deprecated. Use pyspark.sql.Row instead\n",
      "  warnings.warn(\"Using RDD of dict to inferSchema is deprecated. \"\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "df_csv = sqlContext.createDataFrame(dfout, ['company_id', 'company_market_id','company_name','is_dead_company','is_fake_company','is_geo_enabled_company','is_shiftgig_company','is_status_company','tenant_id','updated'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "df1_csv = sqlContext.createDataFrame(dfout, ['company_id', 'company_market_id','company_name','is_dead_company','is_fake_company','is_geo_enabled_company','is_shiftgig_company','is_status_company','tenant_id','updated'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv.coalesce(1).write.format('com.databricks.spark.csv').options(header='true').save('csv_out1.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_csv.coalesce(1).write.format('com.databricks.spark.csv').options(header='true').save('csv1_out.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Glue script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "from decimal import Decimal\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "\n",
    "## @params: [JOB_NAME]\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
    "\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "\n",
    "def unmarshal_dynamodb_json(node):\n",
    "    data = dict({})\n",
    "    data['M'] = node\n",
    "    return _unmarshal_value(data)\n",
    "\n",
    "\n",
    "def _unmarshal_value(node):\n",
    "    if type(node) is not dict:\n",
    "        return node\n",
    "\n",
    "    for key, value in node.items():\n",
    "        # S – String - return string\n",
    "        # N – Number - return int or float (if includes '.')\n",
    "        # B – Binary - not handled\n",
    "        # BOOL – Boolean - return Bool\n",
    "        # NULL – Null - return None\n",
    "        # M – Map - return a dict\n",
    "        # L – List - return a list\n",
    "        # SS – String Set - not handled\n",
    "        # NN – Number Set - not handled\n",
    "        # BB – Binary Set - not handled\n",
    "        key = key.lower()\n",
    "        if key == 'bool':\n",
    "            return value\n",
    "        if key == 'null':\n",
    "            return None\n",
    "        if key == 's':\n",
    "            return value\n",
    "        if key == 'n':\n",
    "            if '.' in str(value):\n",
    "                return Decimal(value)\n",
    "            return int(value)\n",
    "        if key in ['m', 'l']:\n",
    "            if key == 'm':\n",
    "                data = {}\n",
    "                for key1, value1 in value.items():\n",
    "                    if key1.lower() == 'l':\n",
    "                        data = [_unmarshal_value(n) for n in value1]\n",
    "                    else:\n",
    "                        if type(value1) is not dict:\n",
    "                            return _unmarshal_value(value)\n",
    "                        data[key1] = _unmarshal_value(value1)\n",
    "                return data\n",
    "            data = []\n",
    "            for item in value:\n",
    "                data.append(_unmarshal_value(item))\n",
    "            return data\n",
    "\n",
    "\n",
    "\n",
    "connection_options = {\"paths\": [ \"s3://prod-starbase-companybase/2018-04-27-19-28-23/e82ca4a3-6d76-4f9c-bfbd-aaad27cbf161\"]}\n",
    "input_dyf = glueContext.create_dynamic_frame.from_options(connection_type='s3', connection_options=connection_options)\n",
    "company_base = input_dyf.toDF()\n",
    "company_final = sqlContext.read.json(company_base.rdd.map(lambda r : json.dumps(unmarshal_dynamodb_json(json.loads(r['value'])))))\n",
    "company_dynamic = DynamicFrame.fromDF(company_final, glueContext, \"company_dynamic\")\n",
    "\n",
    "## @type: DropNullFields\n",
    "## @args: [transformation_ctx = \"dropnullfields3\"]\n",
    "## @return: dropnullfields3\n",
    "## @inputs: [frame = resolvechoice2]\n",
    "dropnullfields3 = DropNullFields.apply(frame = company_dynamic, transformation_ctx = \"dropnullfields3\")\n",
    "## @type: DataSink\n",
    "## @args: [connection_type = \"s3\", connection_options = {\"path\": \"s3://shiftgig-data-us-east-1-warehouse/orc/company_base/\"}, format = \"orc\", transformation_ctx = \"datasink4\"]\n",
    "## @return: datasink4\n",
    "## @inputs: [frame = dropnullfields3]\n",
    "datasink4 = glueContext.write_dynamic_frame.from_options(frame = dropnullfields3, connection_type = \"s3\", connection_options = {\"path\": \"s3://shiftgig-data-us-east-1-warehouse/orc/company_base/\"}, format = \"orc\", transformation_ctx = \"datasink4\")\n",
    "job.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "from decimal import Decimal\n",
    "from pyspark.sql import SQLContext\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "\n",
    "## @params: [JOB_NAME]\n",
    "#will inherit arguments from the lambda we have yet to make#\n",
    "args = getResolvedOptions(sys.argv,\n",
    "                          ['JOB_NAME',\n",
    "                           's3_object_key',\n",
    "                           'dataset_name',\n",
    "                           ])\n",
    "\n",
    "\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "sqlContext = SQLContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "\n",
    "def unmarshal_dynamodb_json(node):\n",
    "    data = dict({})\n",
    "    data['M'] = node\n",
    "    return _unmarshal_value(data)\n",
    "\n",
    "\n",
    "def _unmarshal_value(node):\n",
    "    if type(node) is not dict:\n",
    "        return node\n",
    "\n",
    "    for key, value in node.items():\n",
    "        # S – String - return string\n",
    "        # N – Number - return int or float (if includes '.')\n",
    "        # B – Binary - not handled\n",
    "        # BOOL – Boolean - return Bool\n",
    "        # NULL – Null - return None\n",
    "        # M – Map - return a dict\n",
    "        # L – List - return a list\n",
    "        # SS – String Set - not handled\n",
    "        # NN – Number Set - not handled\n",
    "        # BB – Binary Set - not handled\n",
    "        key = key.lower()\n",
    "        if key == 'bool':\n",
    "            return value\n",
    "        if key == 'null':\n",
    "            return None\n",
    "        if key == 's':\n",
    "            return value\n",
    "        if key == 'n':\n",
    "            if '.' in str(value):\n",
    "                return Decimal(value)\n",
    "            return int(value)\n",
    "        if key in ['m', 'l']:\n",
    "            if key == 'm':\n",
    "                data = {}\n",
    "                for key1, value1 in value.items():\n",
    "                    if key1.lower() == 'l':\n",
    "                        data = [_unmarshal_value(n) for n in value1]\n",
    "                    else:\n",
    "                        if type(value1) is not dict:\n",
    "                            return _unmarshal_value(value)\n",
    "                        data[key1] = _unmarshal_value(value1)\n",
    "                return data\n",
    "            data = []\n",
    "            for item in value:\n",
    "                data.append(_unmarshal_value(item))\n",
    "            return data\n",
    "\n",
    "\n",
    "dataset = sqlContext.read.text(args['s3_object_key'])\n",
    "dataset_final = sqlContext.read.json(dataset.rdd.map(lambda r : json.dumps(unmarshal_dynamodb_json(json.loads(r['value'])))))\n",
    "dataset_dynamic = DynamicFrame.fromDF(dataset_final, glueContext, \"dataset_dynamic\")\n",
    "\n",
    "## @type: DropNullFields\n",
    "## @args: [transformation_ctx = \"dropnullfields3\"]\n",
    "## @return: dropnullfields3\n",
    "## @inputs: [frame = resolvechoice2]\n",
    "dropnullfields3 = DropNullFields.apply(frame = dataset_dynamic, transformation_ctx = \"dropnullfields3\")\n",
    "## @type: DataSink\n",
    "## @args: [connection_type = \"s3\", connection_options = {\"path\": \"s3://shiftgig-data-us-east-1-warehouse/orc/company_base/\"}, format = \"orc\", transformation_ctx = \"datasink4\"]\n",
    "## @return: datasink4\n",
    "## @inputs: [frame = dropnullfields3]\n",
    "path = \"s3://shiftgig-data-us-east-1-warehouse/orc/{}/\".format(args['dataset_name'])\n",
    "datasink4 = glueContext.write_dynamic_frame.from_options(frame = dropnullfields3, connection_type = \"s3\", connection_options = {\"path\": path}, format = \"orc\", transformation_ctx = \"datasink4\")\n",
    "job.commit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
